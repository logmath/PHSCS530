{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584e4dda",
   "metadata": {},
   "source": [
    "# PHSCS 530\n",
    "## HW 4: Linear Algebra\n",
    "#### Logan Mathews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24140572",
   "metadata": {},
   "source": [
    "import numpy as np."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9967d0",
   "metadata": {},
   "source": [
    "### Problem 1: Condition number, accuracy, and interative updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2eba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.780, 0.563],\n",
    "             [0.913, 0.659]]) # matrix A\n",
    "b = np.array([0.217,0.254]).reshape((2, 1)) # vector b\n",
    "\n",
    "x = np.array([1, -1]).reshape((2, 1)) # exact solution of Ax = b\n",
    "x_alpha = np.array([0.999, -1.001]).reshape((2, 1)) # approximate solution\n",
    "x_beta = np.array([0.341, -0.087]).reshape((2, 1)) # another approximate solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d7052",
   "metadata": {},
   "source": [
    "#### Part A: Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145bd0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The residual for x_alpha is  [[0.001343]\n",
      " [0.001572]]\n",
      "and the residual for r_beta is  [[1.e-06]\n",
      " [0.e+00]]\n"
     ]
    }
   ],
   "source": [
    "r_alpha = b - A@x_alpha\n",
    "r_beta = b - A@x_beta\n",
    "\n",
    "print('The residual for x_alpha is',r_alpha)\n",
    "print('and the residual for r_beta is',r_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd38b4",
   "metadata": {},
   "source": [
    "Curiously, the residual for the less-exact approximate solution ($\\beta$) is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00325587",
   "metadata": {},
   "source": [
    "#### Part B: Condition Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8a31c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The condition number of A is 2193218.9997129813\n",
      "We can expect to lose around k = 6.341081999492477  digits of accuracy.\n"
     ]
    }
   ],
   "source": [
    "U, S, Vh = np.linalg.svd(A) # do SVD on matrix A\n",
    "kappa = S[0]/S[1]\n",
    "print('The condition number of A is',kappa)\n",
    "print('We can expect to lose around k =', np.log10(kappa),' digits of accuracy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee81653",
   "metadata": {},
   "source": [
    "Yikes! That's a wild condition number! $A$ is definitely ill-conditioned. Using the rule of thumb, we can expect to loose up to 6 digits of accuracy in our calculations due to roundoff errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ed7ba",
   "metadata": {},
   "source": [
    "#### Part C: Black-Box Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c878edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error between the exact and solved solution is [[ 2.30988562e-11]\n",
      " [-3.20018456e-11]]\n"
     ]
    }
   ],
   "source": [
    "x_solve = np.linalg.solve(A,b)\n",
    "err = x - x_solve\n",
    "print('The error between the exact and solved solution is',err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0cff8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Ax - b yields [[-2.77555756e-17]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "soln = A@x_solve - b\n",
    "print('Calculating Ax - b yields',soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ae881",
   "metadata": {},
   "source": [
    "It appears that the solution is plenty accurate, as we get within machine precision of zero from $Ax-b$, so the problem seems to be coming from rounding errors in calculating $A$ and $b$. This jives with what we found from the condition number, since we're getting errors in the range of $10^{-11}$ and machine precision is manifesting here as $~10^{-17}$. We lost six digits of precision, which is what we predicted in Part B from the condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc5da4",
   "metadata": {},
   "source": [
    "#### Part D: Iterative Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9359177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "Ainv = np.linalg.inv(A);\n",
    "r_beta = b - A@x_beta\n",
    "deltax = Ainv@(-r_beta)\n",
    "xnew = x_beta - deltax\n",
    "print(xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecc2e5",
   "metadata": {},
   "source": [
    "Analytically, $r_\\beta$ is \n",
    "$$r_\\beta=b-Ax_\\beta\\\\=\\begin{bmatrix}0.217\\\\0.254\\end{bmatrix}-\\begin{bmatrix}0.780&0.563\\\\0.913&0.659\\end{bmatrix}\\begin{bmatrix}0.341\\\\-0.087\\end{bmatrix}\\\\=\\begin{bmatrix}0.217\\\\0.254\\end{bmatrix}-\\begin{bmatrix}0.216999\\\\0.254\\end{bmatrix}\\\\=\\begin{bmatrix}0.000001\\\\0\\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e1cf777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "r_beta_analy = np.array([0.000001,0]).reshape((2, 1)) # vector b\n",
    "deltax = Ainv@(-r_beta)\n",
    "xnew = x_beta - deltax\n",
    "print(xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1041a1",
   "metadata": {},
   "source": [
    "### Problem 2: Sherman-Morrison Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c8fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 3\n",
    "t = 1\n",
    "\n",
    "T = np.array([[E-t, t, 0, 0, 0],\n",
    "              [t, E-t, t, 0, 0],\n",
    "              [0, t, E-t, t, 0],\n",
    "              [0, 0, t, E-t, t],\n",
    "              [0, 0, 0, t, E-t]])\n",
    "\n",
    "C = np.array([[E-t, t, 0, 0, t],\n",
    "              [t, E-t, t, 0, 0],\n",
    "              [0, t, E-t, t, 0],\n",
    "              [0, 0, t, E-t, t],\n",
    "              [t, 0, 0, t, E-t]])\n",
    "\n",
    "Tinv = np.linalg.inv(T) # calculate the inverse of T using numpy linalg inverse routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc532c6",
   "metadata": {},
   "source": [
    "We're going to have to apply Sherman-Morrison twice, since we can't get directly from $T$ to $C$ by adding the outer product of two vectors. So, we start with\n",
    "$D = T + u v^T$ where $u = [t,0,0,0,0]^T$ and $v = [0,0,0,0,t]^T$, hence\n",
    "$$D = T + u v^T \\\\\n",
    "= \\begin{bmatrix}E-t& t& 0& 0& 0\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              0& 0& 0& t& E-t\\end{bmatrix} + \\begin{bmatrix}t\\\\0\\\\0\\\\0\\\\0\\end{bmatrix}\\begin{bmatrix}0&0&0&0&t\\end{bmatrix}\\\\\n",
    "              = \\begin{bmatrix}E-t& t& 0& 0& 0\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              0& 0& 0& t& E-t\\end{bmatrix} + \\begin{bmatrix}0& 0& 0& 0& t\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\end{bmatrix} \\\\= \\begin{bmatrix}E-t& t& 0& 0& t\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              0& 0& 0& t& E-t\\end{bmatrix}$$\n",
    "              \n",
    "Then, we can get to C by having\n",
    "$$C = D + v u^T \\\\= \\begin{bmatrix}E-t& t& 0& 0& t\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              0& 0& 0& t& E-t\\end{bmatrix} + \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\t\\end{bmatrix}\\begin{bmatrix}t&0&0&0&0\\end{bmatrix}\\\\\n",
    "              = \\begin{bmatrix}E-t& t& 0& 0& t\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              0& 0& 0& t& E-t\\end{bmatrix} + \\begin{bmatrix}0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              0& 0& 0& 0& 0\\\\\n",
    "              t& 0& 0& 0& 0\\end{bmatrix} \\\\= \\begin{bmatrix}E-t& t& 0& 0& t\\\\\n",
    "              t& E-t& t& 0& 0\\\\\n",
    "              0& t& E-t& t& 0\\\\\n",
    "              0& 0& t& E-t& t\\\\\n",
    "              t& 0& 0& t& E-t\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df8cd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  5.55111512e-17  0.00000000e+00\n",
      "  -4.44089210e-16]\n",
      " [ 1.11022302e-16 -2.22044605e-16  0.00000000e+00 -1.11022302e-16\n",
      "   1.11022302e-16]\n",
      " [-1.94289029e-16  2.22044605e-16  0.00000000e+00  0.00000000e+00\n",
      "   2.22044605e-16]\n",
      " [ 2.22044605e-16 -1.11022302e-16 -1.11022302e-16  0.00000000e+00\n",
      "  -3.33066907e-16]\n",
      " [-2.22044605e-16  2.22044605e-16 -5.55111512e-17  0.00000000e+00\n",
      "   4.44089210e-16]]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([t,0,0,0,0]).reshape((5, 1)) # vector u\n",
    "v = np.array([0,0,0,0,t]).reshape((5, 1)) # vector v\n",
    "\n",
    "Dinv = Tinv - (Tinv@u@np.transpose(v)@Tinv)/(1 + np.transpose(v)@Tinv@u) # intermediate calculation of Dinv\n",
    "Cinv = Dinv - (Dinv@v@np.transpose(u)@Dinv)/(1 + np.transpose(u)@Dinv@v) # inverse of C\n",
    "\n",
    "Delta_SM = Tinv - Cinv\n",
    "Cinv_direct = np.linalg.inv(C)\n",
    "Delta_direct = Tinv - Cinv_direct\n",
    "print(Delta_direct - Delta_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb2e1b",
   "metadata": {},
   "source": [
    "So, $\\Delta_\\textrm{direct}-\\Delta_\\textrm{S-M}=\\left(T^{-1}-C^{-1}_\\textrm{direct}\\right)-\\left(T^{-1}-C^{-1}_\\textrm{S-M}\\right)\\approx[0]$, that is, the difference between the direct inversion of $C$ and that obtained through Sherman-Morrison are within machine precision of each other, so they are effectively the same. The only difference, then, is that we didn't have to invert $C$ for the Sherman-Morrison method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
